{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanrenyang/TW_TVM/blob/main/tile_matmul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: LD_LIBRARY_PATH=/usr/local/cuda/lib\n",
            "env: PATH=/usr/local/cuda/bin:/usr/bin\n"
          ]
        }
      ],
      "source": [
        "%env LD_LIBRARY_PATH=/usr/local/cuda/lib\n",
        "%env PATH=/usr/local/cuda/bin:/usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4pdug43brs7y"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "import tvm.testing\n",
        "from tvm import te\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AnyVD9V2wXc7"
      },
      "outputs": [],
      "source": [
        "tgt_gpu = tvm.target.Target(target=\"cuda\", host=\"llvm\")\n",
        "gpu_0 = tvm.device(tgt_gpu.kind.name, 0)\n",
        "\n",
        "tgt_cpu = tvm.target.Target(target=\"llvm\", host=\"llvm\")\n",
        "cpu = tvm.device(tgt_cpu.kind.name, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AVLvig5wsqT",
        "outputId": "ea682443-bd6e-4b8d-8802-aff439322eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 4, 6, 7, 8, 12, 13, 14]\n"
          ]
        }
      ],
      "source": [
        "def mask_gen(N: int, N_pruned: int, base = 0):\n",
        "    '''A tool to generate a mask (in the type of python list) given the number of \n",
        "    origial elements `N` and that of remaining elements `N_pruned`'''\n",
        "    mask_keep = list(range(N))\n",
        "    random.shuffle(mask_keep) # shuffle is an in-place operation\n",
        "    mask_keep = mask_keep[ : N_pruned]\n",
        "    mask_keep.sort()\n",
        "    for i, _ in enumerate(mask_keep):\n",
        "        mask_keep[i] = mask_keep[i]  + base\n",
        "    return mask_keep\n",
        "'''\n",
        "In the cuda version of TW, `mask[i]+i` is the real index relative to the beginning of its block of the corresponding element\n",
        "\n",
        "In this tvm version, we set `mask[i]` to be the real index instead of `mask[i]+i`.\n",
        "'''\n",
        "print(mask_gen(16, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkPZNprpxXaS",
        "outputId": "6370cb13-171f-402b-daf4-ae28191d8de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "block_num_verify: 4\n",
            "N_original_perBlock_verify: 4\n",
            "mask_k_list_verify: [[1, 4, 6, 7], [0, 1, 2, 5], [2, 4, 5, 7], [2, 3, 5, 6]]\n",
            "mask_n_list_verify: [[1, 2], [0, 3], [1, 3], [0, 1]]\n",
            "\n",
            "Block of dense matrix:\n",
            " [[0.7338991  0.9039647  0.64008491 0.93956132]\n",
            " [0.30641148 0.19852933 0.52610446 0.96497124]\n",
            " [0.83806996 0.41329319 0.86986179 0.64806269]\n",
            " [0.01819067 0.86111985 0.45010897 0.7578844 ]\n",
            " [0.4083653  0.53162131 0.32541112 0.35413239]\n",
            " [0.54418759 0.51079587 0.32296146 0.19155029]\n",
            " [0.47227041 0.64321622 0.25844482 0.73020564]\n",
            " [0.82890208 0.53941138 0.32041523 0.42371766]]\n",
            "\n",
            "Block of sparse matrix\n",
            " [[0.19852933 0.52610446]\n",
            " [0.53162131 0.32541112]\n",
            " [0.64321622 0.25844482]\n",
            " [0.53941138 0.32041523]]\n"
          ]
        }
      ],
      "source": [
        "def get_B_Stream(B, mask_k_list, mask_n_list, block_num, N_pruned_perBlock, K_pruned, N_original):\n",
        "    ''' To generate a python list of B tiles with type of `list(tvm.nd.NDArray)`\n",
        "    Parameters:\n",
        "    * B(tvm.nd.NDArray): the undivided matrix B\n",
        "    * mask_k_list: mask of K dimension of each block with type of `list(list(int))`\n",
        "    * mask_n_list: mask of N dimension of each block with type of `list(list(int))`\n",
        "    * block_num: the number of blocks in B\n",
        "    * N_pruned_perBlock: the number of remaining elements in the N dimension of each block\n",
        "    * K_pruned: the number of remaining elements in the K dimension of each block\n",
        "    * N_original: the number of elements in the N dimension of the undivided matrix\n",
        "                    used to compute the offset of each block in the N dimension \n",
        "    '''\n",
        "    B_transposed_tiled_list = []\n",
        "    for bn in range(block_num):\n",
        "        mask_k = mask_k_list[bn]\n",
        "        mask_n = mask_n_list[bn]\n",
        "        \n",
        "        dst = np.zeros((N_pruned_perBlock, K_pruned))\n",
        "        for i in range(K_pruned):\n",
        "            for j in range(N_pruned_perBlock):\n",
        "                idx_col = mask_k[i] \n",
        "                idx_row = mask_n[j] + N_original * bn\n",
        "                dst[j, i] = B[idx_col, idx_row]\n",
        "        B_transposed_tiled_list.append(dst)\n",
        "    return B_transposed_tiled_list\n",
        "\n",
        "'''`_verify` means the variable is used for unit test'''\n",
        "B_verify = np.random.random((8, 16))\n",
        "K_verify = 8\n",
        "N_verify = 16\n",
        "\n",
        "K_pruned_verify = 4 # the numbers of remaining elements in the K dimension of each block are the same\n",
        "N_pruned_global_verify = 8 # but those in the N dimension are different\n",
        "\n",
        "tilesize_verify = 2\n",
        "\n",
        "block_num_verify = (N_pruned_global_verify+tilesize_verify-1)//tilesize_verify\n",
        "N_original_perBlock_verify = N_verify // block_num_verify\n",
        "print(\"block_num_verify:\",block_num_verify)\n",
        "print(\"N_original_perBlock_verify:\", N_original_perBlock_verify)\n",
        "\n",
        "mask_k_list_verify = [mask_gen(K_verify, K_pruned_verify) for _ in range(block_num_verify)]\n",
        "mask_n_list_verify = [mask_gen(N_original_perBlock_verify, tilesize_verify) for _ in range(block_num_verify)]\n",
        "print(\"mask_k_list_verify:\", mask_k_list_verify)\n",
        "print(\"mask_n_list_verify:\", mask_n_list_verify)\n",
        "\n",
        "B_transposed_tiled_list = get_B_Stream(B_verify, mask_k_list_verify,\\\n",
        "                                                mask_n_list_verify, block_num_verify, \\\n",
        "                                                tilesize_verify, K_pruned_verify, N_original_perBlock_verify)\n",
        "block_to_check = 0# number of block to check, from 0 to 3\n",
        "print(\"\\nBlock of dense matrix:\\n\", B_verify[0:8, 8*block_to_check:8*block_to_check+4])\n",
        "print(\"\\nBlock of sparse matrix\\n\", B_transposed_tiled_list[block_to_check].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJpp0CGLBzdj",
        "outputId": "cdcb8c6d-2cae-4d0a-8e5c-394dbaf2ba5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Default schedule\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for j, i in T.grid(32, 1024):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j * 1024 + i] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_1: T.int32 = j * 1024 + i\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + i] * B_tiled_1[j * 128 + k]\n",
            "\n",
            "After split\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for i_outer, j_inner, i_inner in T.grid(32, 32, 32):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j_inner * 1024 + i_outer * 32 + i_inner] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_2: T.int32 = i_outer * 32\n",
            "                cse_var_1: T.int32 = j_inner * 1024 + cse_var_2 + i_inner\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + cse_var_2 + i_inner] * B_tiled_1[j_inner * 128 + k]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(schedule(0x2e68f70),\n",
              " [Tensor(shape=[32, 1024], op.name=C_transposed_skipped),\n",
              "  Tensor(shape=[2048, 1024], op.name=A_transposed),\n",
              "  Tensor(shape=[32, 128], op.name=B_tiled),\n",
              "  Tensor(shape=[128], op.name=mask_k)])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M = 1024\n",
        "K = 1024\n",
        "N = 1024\n",
        "tile_size = 32\n",
        "K_pruned = 128\n",
        "N_pruned = 128\n",
        "\n",
        "\n",
        "block_num = (N_pruned + tile_size - 1) // tile_size\n",
        "N_ori = N//block_num\n",
        "def get_tiled_matmul_kernel(cuda=False):\n",
        "    '''TW Tiled-Gemm kernel\n",
        "    Input of the kernel: \n",
        "    * A_transposed\n",
        "    * B_transposed_tiled\n",
        "    * mask_k\n",
        "    Output of the kernel:\n",
        "    * '''\n",
        "    A_transposed = te.placeholder((K, M), name='A_transposed')\n",
        "    mask_k = te.placeholder((K_pruned,), name='mask_k', dtype='int')\n",
        "    B_transposed_tiled = te.placeholder((tile_size, K_pruned), name='B_tiled')\n",
        "    # mask_k = [0 for i in range(K_pruned)]\n",
        "    # mask_n = te.placeholder((N_dim,), name=\"mask_n\")\n",
        "    \n",
        "    A_transposed_skipped = te.compute((K_pruned, M), lambda i,j: A_transposed[mask_k[i], j], name='A_skipped')\n",
        "\n",
        "    k = te.reduce_axis((0, K_pruned), name='k')\n",
        "    C_transposed_skipped = te.compute((tile_size, M),lambda j,i: te.sum(A_transposed_skipped[k, i]*B_transposed_tiled[j, k], axis=k),name='C_transposed_skipped')\n",
        "    \n",
        "    s = te.create_schedule(C_transposed_skipped.op)\n",
        "\n",
        "    '''schedule'''\n",
        "    print('\\nDefault schedule')\n",
        "    print(tvm.lower(s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k], simple_mode=True))\n",
        "\n",
        "    yo, xo, yi, xi = s[C_transposed_skipped].tile(C_transposed_skipped.op.axis[0], C_transposed_skipped.op.axis[1], x_factor = 32, y_factor=32)\n",
        "    print('\\nAfter split')\n",
        "    print(tvm.lower(s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k], simple_mode=True))\n",
        "\n",
        "    if cuda:\n",
        "        s[C_transposed_skipped].bind(yo, te.thread_axis(\"blockIdx.y\"))\n",
        "        s[C_transposed_skipped].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
        "        s[C_transposed_skipped].bind(yi, te.thread_axis(\"threadIdx.y\"))\n",
        "        s[C_transposed_skipped].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
        "        print('\\nLaunch threads')\n",
        "        print(tvm.lower(s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k], simple_mode=True))\n",
        "    \n",
        "    return s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k]\n",
        "get_tiled_matmul_kernel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Default schedule\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for j, i in T.grid(32, 1024):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j * 1024 + i] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_1: T.int32 = j * 1024 + i\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + i] * B_tiled_1[j * 128 + k]\n",
            "\n",
            "After split\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for i_outer, j_inner, i_inner in T.grid(32, 32, 32):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j_inner * 1024 + i_outer * 32 + i_inner] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_2: T.int32 = i_outer * 32\n",
            "                cse_var_1: T.int32 = j_inner * 1024 + cse_var_2 + i_inner\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + cse_var_2 + i_inner] * B_tiled_1[j_inner * 128 + k]\n",
            "C_transposed_skipped [32, 1024]\n",
            "A_transposed [2048, 1024]\n",
            "B_tiled [32, 128]\n",
            "mask_k [128]\n"
          ]
        }
      ],
      "source": [
        "'''Testing cpu'''\n",
        "schedule, placeholders = get_tiled_matmul_kernel(cuda=False)\n",
        "for ph in placeholders:\n",
        "  print(ph.op.name, ph.shape)\n",
        "\n",
        "tiled_matmul_kernel = tvm.build(schedule, placeholders, target=tgt_cpu, name=\"tiled_matmul\")  \n",
        "A_transposed_data = tvm.nd.array(np.random.uniform(size=(K, M)).astype(placeholders[1].dtype), cpu)\n",
        "B_transposed_tiled_data = tvm.nd.array(np.random.uniform(size=(tile_size, K_pruned)).astype(placeholders[2].dtype), cpu)\n",
        "mask_k_data = tvm.nd.array(np.array(mask_gen(K, K_pruned)).astype(placeholders[3].dtype), cpu)\n",
        "\n",
        "C_transposed_skipped_data = tvm.nd.array(np.random.uniform(size=(tile_size, M)).astype(placeholders[0].dtype), cpu)\n",
        "\n",
        "tiled_matmul_kernel(C_transposed_skipped_data, A_transposed_data, B_transposed_tiled_data, mask_k_data)\n",
        "\n",
        "def tiled_matmul_test(A_transposed, B_transposed_tiled, mask_k):\n",
        "    A_transposed_skipped = np.zeros((K_pruned, M))\n",
        "    for i in range(K_pruned):\n",
        "        for j in range(M):\n",
        "            A_transposed_skipped[i, j] = A_transposed[mask_k[i], j]\n",
        "    C_transposed_skipped = (A_transposed_skipped.T @ B_transposed_tiled.T).T\n",
        "    return C_transposed_skipped\n",
        "\n",
        "tvm.testing.assert_allclose(C_transposed_skipped_data.numpy(), tiled_matmul_test(A_transposed_data.numpy(), B_transposed_tiled_data.numpy(), mask_k_data.numpy()), 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "block_num 16\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n"
          ]
        },
        {
          "ename": "AssertionError",
          "evalue": "\nNot equal to tolerance rtol=1e-06, atol=1e-07\n\nMismatched elements: 703480 / 1048576 (67.1%)\nMax absolute difference: 155.375\nMax relative difference: 1.5351117\n x: array([[68.44, 66.06,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [62.38, 64.44,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [62.  , 64.6 ,  0.  , ...,  0.  ,  0.  ,  0.  ],...\n y: array([[68.688385, 65.97537 ,  0.      , ..., 66.74563 ,  0.      ,\n        64.945145],\n       [62.37388 , 64.549484,  0.      , ..., 69.41834 ,  0.      ,...",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 136\u001b[0m\n\u001b[1;32m    131\u001b[0m     tvm\u001b[39m.\u001b[39mtesting\u001b[39m.\u001b[39massert_allclose(C_transposed_test\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mT, A_transposed_test\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m B_transposed_pruned\u001b[39m.\u001b[39mT, \u001b[39m1e-6\u001b[39m)\n\u001b[1;32m    133\u001b[0m     \u001b[39mprint\u001b[39m(tvm\u001b[39m.\u001b[39mlower(s, [C_transposed, A_transposed, B_transposed_packed, mask_k, mask_n], simple_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[0;32m--> 136\u001b[0m get_tw_kernel(M, N, K, K_pruned_max, N_pruned_global, tile_size, \u001b[39mFalse\u001b[39;49;00m)\n",
            "Cell \u001b[0;32mIn[4], line 131\u001b[0m, in \u001b[0;36mget_tw_kernel\u001b[0;34m(M, N, K, K_pruned_max, N_pruned_global, tile_size, cuda)\u001b[0m\n\u001b[1;32m    127\u001b[0m mask_n_test \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mnd\u001b[39m.\u001b[39marray(mask_n_test)\n\u001b[1;32m    129\u001b[0m tw_kernel(C_transposed_test, A_transposed_test, B_transposed_packed_test, mask_k_test, mask_n_test)\n\u001b[0;32m--> 131\u001b[0m tvm\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_allclose(C_transposed_test\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mT, A_transposed_test\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m B_transposed_pruned\u001b[39m.\u001b[39;49mT, \u001b[39m1e-6\u001b[39;49m)\n\u001b[1;32m    133\u001b[0m \u001b[39mprint\u001b[39m(tvm\u001b[39m.\u001b[39mlower(s, [C_transposed, A_transposed, B_transposed_packed, mask_k, mask_n], simple_mode\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tvm-0.12.dev176+g18b7dc1dd-py3.8-linux-x86_64.egg/tvm/testing/utils.py:120\u001b[0m, in \u001b[0;36massert_allclose\u001b[0;34m(actual, desired, rtol, atol)\u001b[0m\n\u001b[1;32m    118\u001b[0m desired \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(desired)\n\u001b[1;32m    119\u001b[0m np\u001b[39m.\u001b[39mtesting\u001b[39m.\u001b[39massert_allclose(actual\u001b[39m.\u001b[39mshape, desired\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> 120\u001b[0m np\u001b[39m.\u001b[39;49mtesting\u001b[39m.\u001b[39;49massert_allclose(actual, desired, rtol\u001b[39m=\u001b[39;49mrtol, atol\u001b[39m=\u001b[39;49matol, verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/miniconda3/envs/tvm-build-binding/lib/python3.8/contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     74\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 75\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/testing/_private/utils.py:862\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    858\u001b[0m         err_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(remarks)\n\u001b[1;32m    859\u001b[0m         msg \u001b[39m=\u001b[39m build_err_msg([ox, oy], err_msg,\n\u001b[1;32m    860\u001b[0m                             verbose\u001b[39m=\u001b[39mverbose, header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m    861\u001b[0m                             names\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m), precision\u001b[39m=\u001b[39mprecision)\n\u001b[0;32m--> 862\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    863\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtraceback\u001b[39;00m\n",
            "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=1e-06, atol=1e-07\n\nMismatched elements: 703480 / 1048576 (67.1%)\nMax absolute difference: 155.375\nMax relative difference: 1.5351117\n x: array([[68.44, 66.06,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [62.38, 64.44,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [62.  , 64.6 ,  0.  , ...,  0.  ,  0.  ,  0.  ],...\n y: array([[68.688385, 65.97537 ,  0.      , ..., 66.74563 ,  0.      ,\n        64.945145],\n       [62.37388 , 64.549484,  0.      , ..., 69.41834 ,  0.      ,..."
          ]
        }
      ],
      "source": [
        "# N_pruned is the number of remaining entries in N dimension\n",
        "# TODO: change K_pruned to a layer-wise configuration\n",
        "big_test = True\n",
        "if big_test:\n",
        "    '''test for accuracy'''\n",
        "    M = 1024\n",
        "    N = 1024\n",
        "    K = 512\n",
        "    tile_size = 32\n",
        "    K_pruned_max = 256\n",
        "    N_pruned_global = 512\n",
        "else:\n",
        "    '''test for visualization'''\n",
        "    M = 16\n",
        "    N = 16\n",
        "    K = 32\n",
        "    tile_size =  2\n",
        "    K_pruned_max = 8\n",
        "    N_pruned_global = 8\n",
        "\n",
        "\n",
        "\n",
        "#int, N: int, K:int, K_pruned_max: int, N_pruned:int, tile_size:int,\n",
        "def get_tw_kernel( M: int, N: int, K:int, K_pruned_max: int, N_pruned_global:int, tile_size:int, cuda:bool=False):\n",
        "    '''TW Tiled-Gemm kernel\n",
        "    Input of the kernel: \n",
        "    * A: K*M\n",
        "    * B: (block_num, tile_size, K_pruned_max)\n",
        "    * C: N*M\n",
        "    * mask_k: (block_num, K_pruned_max)\n",
        "    * mask_n: (block_num, tile_size)\n",
        "    * block_num\n",
        "    Output of the kernel:\n",
        "    * '''\n",
        "    dtype = 'float16'\n",
        "    block_num = (N_pruned_global + tile_size - 1)//tile_size\n",
        "    print('block_num', block_num)\n",
        "    N_ori_per_block = N // block_num\n",
        "\n",
        "    A_transposed = te.placeholder((K, M), name='A_transposed')\n",
        "    B_transposed_packed = te.placeholder((block_num, tile_size, K_pruned_max), name='B_transposed_packed')\n",
        "    \n",
        "\n",
        "    mask_k = te.placeholder((block_num, K_pruned_max), name='mask_k', dtype='int')\n",
        "    mask_n = te.placeholder((block_num, tile_size), name='mask_n', dtype='int') # \n",
        "\n",
        "    A_transposed_skipped = te.compute((block_num, K_pruned_max, M), lambda bn, i, j: A_transposed[mask_k[bn, i], j].astype(dtype), name='A_transposed_skipped')\n",
        "    \n",
        "    k = te.reduce_axis((0, K_pruned_max), name='k')\n",
        "    C_transposed_skipped = te.compute((block_num, tile_size, M), lambda bn, j, i: te.sum(A_transposed_skipped[bn, k, i] * B_transposed_packed[bn, j, k].astype(dtype), axis=k) , name='C_transposed_skipped')\n",
        "\n",
        "    def write_C_to_sparse(data, mask_n, out):\n",
        "        '''\n",
        "        data: shape of (block_num, tile_size, M)\n",
        "        mask_n: shape of (block_num, tile_size)\n",
        "        '''\n",
        "        irb = tvm.tir.ir_builder.create()\n",
        "        data_ptr = irb.buffer_ptr(data)\n",
        "        mask_n_ptr = irb.buffer_ptr(mask_n)\n",
        "        out_ptr = irb.buffer_ptr(out)\n",
        "\n",
        "        assert data.shape[0]==mask_n.shape[0], 'block_num mismatches'\n",
        "        block_num = data.shape[0]\n",
        "        assert data.shape[1]==mask_n.shape[1], 'tile_size mismatches'\n",
        "        tile_size = data.shape[1]\n",
        "        \n",
        "        N = out.shape[0]\n",
        "        M = out.shape[1]\n",
        "\n",
        "        with irb.for_range(0, N, kind='serial', name='n') as n:\n",
        "            with irb.for_range(0, M, kind='serial', name='m') as m:\n",
        "                out_ptr[n * M + m] = tvm.tir.generic.cast(0, data.dtype)\n",
        "\n",
        "        with irb.for_range(0, block_num, kind='serial', name='bn') as bn:\n",
        "            with irb.for_range(0, tile_size, kind='serial', name='ts') as ts:\n",
        "                with irb.for_range(0, M, kind='serial', name='col') as col:\n",
        "                    out_ptr[(tile_size * bn + mask_n_ptr[ts]) * M + col] += data_ptr[bn * tile_size * M + ts * M + col]\n",
        "        return irb.get()\n",
        "        \n",
        "    C_transposed = te.extern((N, M),\n",
        "                             [C_transposed_skipped, mask_n],\n",
        "                             lambda ins, outs: write_C_to_sparse(ins[0], ins[1], outs[0]),\n",
        "                             tag='write_C_to_sparse',\n",
        "                             dtype=C_transposed_skipped.dtype,\n",
        "                             name='C_transposed',\n",
        "                             )\n",
        "    \n",
        "    s = te.create_schedule(C_transposed.op)\n",
        "\n",
        "    '''testing cpu'''\n",
        "    tw_kernel = tvm.build(s, [C_transposed, A_transposed, B_transposed_packed, mask_k, mask_n], tgt_cpu, name='tiled_matmul')\n",
        "\n",
        "    A_transposed_test = np.random.random((K, M)).astype(A_transposed.dtype)\n",
        "    B_transposed_test = np.random.random((N, K)).astype(B_transposed_packed.dtype)\n",
        "    C_transposed_test = np.zeros((N, M)).astype(C_transposed.dtype)\n",
        "\n",
        "    # generate mask_n and mask_k\n",
        "    mask_n_test = np.zeros((block_num, tile_size)).astype(mask_n.dtype)\n",
        "    for row in range(block_num):\n",
        "        mask_n_test[row, :] = np.random.choice(N_ori_per_block, tile_size, replace=False)\n",
        "    \n",
        "    mask_k_test = np.zeros((block_num, K_pruned_max)).astype(mask_k.dtype)\n",
        "    for row in range(block_num):\n",
        "        mask_k_test[row, :] = np.random.choice(K, K_pruned_max, replace=False)\n",
        "    \n",
        "    \n",
        "    # apply mask to B_transposed\n",
        "    B_transposed_pruned = np.zeros((N, K)).astype(B_transposed_packed.dtype)\n",
        "    for bn in range(block_num):\n",
        "        print(bn)\n",
        "        for ts in range(tile_size):\n",
        "            for k in range(K_pruned_max):\n",
        "                B_transposed_pruned[(bn*N_ori_per_block+mask_n_test[bn, ts]), mask_k_test[bn, k]] = B_transposed_test[(bn*N_ori_per_block+mask_n_test[bn, ts]), mask_k_test[bn, k]]\n",
        "    \n",
        "    # transpose B to B_packed\n",
        "    B_transposed_packed_test = np.zeros((block_num, tile_size, K_pruned_max)).astype(B_transposed_packed.dtype)\n",
        "    for bn in range(block_num):\n",
        "        print(bn)\n",
        "        for ts in range(tile_size):\n",
        "            for k in range(K_pruned_max):\n",
        "                B_transposed_packed_test[bn, ts, k] = B_transposed_test[(bn*N_ori_per_block+mask_n_test[bn, ts]), mask_k_test[bn, k]]\n",
        "\n",
        "    A_transposed_test = tvm.nd.array(A_transposed_test)\n",
        "    B_transposed_packed_test = tvm.nd.array(B_transposed_packed_test)\n",
        "    C_transposed_test = tvm.nd.array(C_transposed_test)\n",
        "    mask_k_test = tvm.nd.array(mask_k_test)\n",
        "    mask_n_test = tvm.nd.array(mask_n_test)\n",
        "\n",
        "    tw_kernel(C_transposed_test, A_transposed_test, B_transposed_packed_test, mask_k_test, mask_n_test)\n",
        "\n",
        "    tvm.testing.assert_allclose(C_transposed_test.numpy().T, A_transposed_test.numpy().T @ B_transposed_pruned.T, 1e-6)\n",
        "    \n",
        "    print(tvm.lower(s, [C_transposed, A_transposed, B_transposed_packed, mask_k, mask_n], simple_mode=True))\n",
        "    \n",
        "    \n",
        "get_tw_kernel(M, N, K, K_pruned_max, N_pruned_global, tile_size, False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOsXw8Qb1X5fa8km6wQCAiz",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tvm-build-binding",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "400f399043f899df7d546bcb6063b1538c013427ed509bbd25e6c99d0cce4f96"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
