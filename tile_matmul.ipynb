{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsXw8Qb1X5fa8km6wQCAiz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanrenyang/TW_TVM/blob/main/tile_matmul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "# Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n",
        "# you must request a Google Colab instance with a GPU by going to Runtime ->\n",
        "# Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n",
        "# source, see see https://tvm.apache.org/docs/install/from_source.html\n",
        "pip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USsQNSwnwGkG",
        "outputId": "8f57e478-6cf7-4bf3-95eb-392a28c9d63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://tlcpack.ai/wheels\n",
            "Requirement already satisfied: tlcpack-nightly-cu113 in /usr/local/lib/python3.8/dist-packages (0.12.dev240+g142ea59fb)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (22.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (1.7.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (5.4.8)\n",
            "Requirement already satisfied: numpy<=1.23 in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (1.21.6)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (2.2.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.8/dist-packages (from tlcpack-nightly-cu113) (6.0.4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pdug43brs7y"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "import tvm.testing\n",
        "from tvm import te\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgt_gpu = tvm.target.Target(target=\"cuda\", host=\"llvm\")\n",
        "gpu_0 = tvm.device(tgt_gpu.kind.name, 0)\n",
        "\n",
        "tgt_cpu = tvm.target.Target(target=\"llvm\", host=\"llvm\")\n",
        "cpu = tvm.device(tgt_cpu.kind.name, 0)"
      ],
      "metadata": {
        "id": "AnyVD9V2wXc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = 1024\n",
        "K = 2048\n",
        "N = 1024\n",
        "N_dim = 32\n",
        "K_pruned = 128\n",
        "N_pruned = 128\n",
        "\n",
        "sparsity = \n",
        "block_num = (N_pruned + N_dim - 1) // N_dim\n",
        "N_ori = N//block_num"
      ],
      "metadata": {
        "id": "EzSWohp5waHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_gen(N: int, N_pruned: int, base = 0):\n",
        "  '''A tool to generate a mask (in the type of python list) given the number of \n",
        "  origial elements `N` and that of remaining elements `N_pruned`'''\n",
        "  mask_keep = list(range(N))\n",
        "  random.shuffle(mask_keep) # shuffle is an in-place operation\n",
        "  mask_keep = mask_keep[ : N_pruned]\n",
        "  mask_keep.sort()\n",
        "  for i, _ in enumerate(mask_keep):\n",
        "      mask_keep[i] = mask_keep[i]  + base\n",
        "  return mask_keep\n",
        "'''\n",
        "In the cuda version of TW, `mask[i]+i` is the real index relative to the beginning of its block of the corresponding element\n",
        "\n",
        "In this tvm version, we set `mask[i]` to be the real index instead of `mask[i]+i`.\n",
        "'''\n",
        "print(mask_gen(16, 8))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AVLvig5wsqT",
        "outputId": "ea682443-bd6e-4b8d-8802-aff439322eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 3, 4, 9, 10, 11, 12, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_B_Stream(B, mask_k_list, mask_n_list, block_num, N_pruned_perBlock, K_pruned, N_original):\n",
        "  ''' To generate a python list of B tiles with type of `list(tvm.nd.NDArray)`\n",
        "  Parameters:\n",
        "  * B(tvm.nd.NDArray): the undivided matrix B\n",
        "  * mask_k_list: mask of K dimension of each block with type of `list(list(int))`\n",
        "  * mask_n_list: mask of N dimension of each block with type of `list(list(int))`\n",
        "  * block_num: the number of blocks in B\n",
        "  * N_pruned_perBlock: the number of remaining elements in the N dimension of each block\n",
        "  * K_pruned: the number of remaining elements in the K dimension of each block\n",
        "  * N_original: the number of elements in the N dimension of the undivided matrix\n",
        "                used to compute the offset of each block in the N dimension \n",
        "  '''\n",
        "  B_transposed_tiled_list = []\n",
        "  for bn in range(block_num):\n",
        "      mask_k = mask_k_list[bn]\n",
        "      mask_n = mask_n_list[bn]\n",
        "      \n",
        "      dst = np.zeros((N_pruned_perBlock, K_pruned))\n",
        "      for i in range(K_pruned):\n",
        "          for j in range(N_pruned_perBlock):\n",
        "              idx_col = mask_k[i] \n",
        "              idx_row = mask_n[j] + N_original * bn\n",
        "              dst[j, i] = B[idx_col, idx_row]\n",
        "      B_transposed_tiled_list.append(dst)\n",
        "  return B_transposed_tiled_list\n",
        "\n",
        "'''`_verify` means the variable is used for unit test'''\n",
        "B_verify = np.random.random((8, 16))\n",
        "K_verify = 8\n",
        "N_verify = 16\n",
        "\n",
        "K_pruned_verify = 4 # the numbers of remaining elements in the K dimension of each block are the same\n",
        "N_pruned_global_verify = 8 # but those in the N dimension are different\n",
        "\n",
        "tilesize_verify = 2\n",
        "\n",
        "block_num_verify = (N_pruned_global_verify+tilesize_verify-1)//tilesize_verify\n",
        "N_original_perBlock_verify = N_verify // block_num_verify\n",
        "print(\"block_num_verify:\",block_num_verify)\n",
        "print(\"N_original_perBlock_verify:\", N_original_perBlock_verify)\n",
        "\n",
        "mask_k_list_verify = [mask_gen(K_verify, K_pruned_verify) for _ in range(block_num_verify)]\n",
        "mask_n_list_verify = [mask_gen(N_original_perBlock_verify, tilesize_verify) for _ in range(block_num_verify)]\n",
        "print(\"mask_k_list_verify:\", mask_k_list_verify)\n",
        "print(\"mask_n_list_verify:\", mask_n_list_verify)\n",
        "\n",
        "B_transposed_tiled_list = get_B_Stream(B_verify, mask_k_list_verify,\\\n",
        "                                                mask_n_list_verify, block_num_verify, \\\n",
        "                                                tilesize_verify, K_pruned_verify, N_original_perBlock_verify)\n",
        "block_to_check = 0# number of block to check, from 0 to 3\n",
        "print(\"\\nBlock of dense matrix:\\n\", B_verify[0:8, 8*block_to_check:8*block_to_check+4])\n",
        "print(\"\\nBlock of sparse matrix\\n\", B_transposed_tiled_list[block_to_check].T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkPZNprpxXaS",
        "outputId": "6370cb13-171f-402b-daf4-ae28191d8de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "block_num_verify: 4\n",
            "N_original_perBlock_verify: 4\n",
            "mask_k_list_verify: [[0, 1, 2, 4], [3, 4, 6, 7], [0, 1, 2, 4], [0, 1, 2, 7]]\n",
            "mask_n_list_verify: [[1, 3], [1, 3], [0, 3], [1, 2]]\n",
            "\n",
            "Block of dense matrix:\n",
            " [[0.82392874 0.2529673  0.53781596 0.3805554 ]\n",
            " [0.55990472 0.2779939  0.36911807 0.63154789]\n",
            " [0.34482586 0.02511602 0.22861381 0.3542492 ]\n",
            " [0.48808761 0.97802291 0.14443363 0.83602811]\n",
            " [0.26096124 0.26661773 0.0823008  0.2562803 ]\n",
            " [0.74169749 0.71631852 0.64701425 0.95544594]\n",
            " [0.27538455 0.34235265 0.35923464 0.26231561]\n",
            " [0.02141543 0.3525766  0.7727293  0.66557609]]\n",
            "\n",
            "Block of sparse matrix\n",
            " [[0.2529673  0.3805554 ]\n",
            " [0.2779939  0.63154789]\n",
            " [0.02511602 0.3542492 ]\n",
            " [0.26661773 0.2562803 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tiled_matmul_kernel():\n",
        "  '''TW Tiled-Gemm kernel\n",
        "  Input of the kernel: \n",
        "  * A_transposed\n",
        "  * B_transposed_tiled\n",
        "  * mask_k\n",
        "  Output of the kernel:\n",
        "  * '''\n",
        "  A_transposed = te.placeholder((K, M), name='A_transposed')\n",
        "  mask_k = te.placeholder((K_pruned,), name='mask_k', dtype='int')\n",
        "  B_transposed_tiled = te.placeholder((N_dim, K_pruned), name='B_tiled')\n",
        "  # mask_k = [0 for i in range(K_pruned)]\n",
        "  # mask_n = te.placeholder((N_dim,), name=\"mask_n\")\n",
        "  \n",
        "  A_transposed_skipped = te.compute((K_pruned, M), lambda i,j: A_transposed[mask_k[i], j], name='A_skipped')\n",
        "\n",
        "  k = te.reduce_axis((0, K_pruned), name='k')\n",
        "  C_transposed_skipped = te.compute((N_dim, M),lambda j,i: te.sum(A_transposed_skipped[k, i]*B_transposed_tiled[j, k], axis=k),name='C_transposed_skipped')\n",
        "  \n",
        "  s = te.create_schedule(C_transposed_skipped.op)\n",
        "\n",
        "  return s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k]\n",
        "\n",
        "'''Testing'''\n",
        "schedule, placeholders = get_tiled_matmul_kernel()\n",
        "for ph in placeholders:\n",
        "  print(ph.op.name, ph.shape)\n",
        "\n",
        "print(tvm.lower(schedule, placeholders, simple_mode=True))\n",
        "\n",
        "tiled_matmul_kernel = tvm.build(schedule, placeholders, target=tgt_cpu, name=\"tiled_matmul\")  \n",
        "A_transposed_data = tvm.nd.array(np.random.uniform(size=(K, M)).astype(placeholders[1].dtype), cpu)\n",
        "B_transposed_tiled_data = tvm.nd.array(np.random.uniform(size=(N_dim, K_pruned)).astype(placeholders[2].dtype), cpu)\n",
        "mask_k_data = tvm.nd.array(np.array(mask_gen(K, K_pruned)).astype(placeholders[3].dtype), cpu)\n",
        "\n",
        "C_transposed_skipped_data = tvm.nd.array(np.random.uniform(size=(N_dim, M)).astype(placeholders[0].dtype), cpu)\n",
        "\n",
        "tiled_matmul_kernel(C_transposed_skipped_data, A_transposed_data, B_transposed_tiled_data, mask_k_data)\n",
        "\n",
        "def tiled_matmul_test(A_transposed, B_transposed_tiled, mask_k):\n",
        "    A_transposed_skipped = np.zeros((K_pruned, M))\n",
        "    for i in range(K_pruned):\n",
        "        for j in range(M):\n",
        "            A_transposed_skipped[i, j] = A_transposed[mask_k[i], j]\n",
        "    C_transposed_skipped = (A_transposed_skipped.T @ B_transposed_tiled.T).T\n",
        "    return C_transposed_skipped\n",
        "\n",
        "tvm.testing.assert_allclose(C_transposed_skipped_data.numpy(), tiled_matmul_test(A_transposed_data.numpy(), B_transposed_tiled_data.numpy(), mask_k_data.numpy()), 1e-6)\n"
      ],
      "metadata": {
        "id": "CJpp0CGLBzdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcb8c6d-2cae-4d0a-8e5c-394dbaf2ba5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C_transposed_skipped [32, 1024]\n",
            "A_transposed [2048, 1024]\n",
            "B_tiled [32, 128]\n",
            "mask_k [128]\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for j, i in T.grid(32, 1024):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j * 1024 + i] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_1: T.int32 = j * 1024 + i\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + i] * B_tiled_1[j * 128 + k]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tiled_matmul_kernel():\n",
        "  '''TW Tiled-Gemm kernel\n",
        "  Input of the kernel: \n",
        "  * A_transposed\n",
        "  * B_transposed_tiled\n",
        "  * mask_k\n",
        "  Output of the kernel:\n",
        "  * '''\n",
        "  A_transposed = te.placeholder((K, M), name='A_transposed')\n",
        "  mask_k = te.placeholder((K_pruned,), name='mask_k', dtype='int')\n",
        "  B_transposed_tiled = te.placeholder((N_dim, K_pruned), name='B_tiled')\n",
        "  # mask_k = [0 for i in range(K_pruned)]\n",
        "  # mask_n = te.placeholder((N_dim,), name=\"mask_n\")\n",
        "  \n",
        "  A_transposed_skipped = te.compute((K_pruned, M), lambda i,j: A_transposed[mask_k[i], j], name='A_skipped')\n",
        "\n",
        "  k = te.reduce_axis((0, K_pruned), name='k')\n",
        "  C_transposed_skipped = te.compute((N_dim, M),lambda j,i: te.sum(A_transposed_skipped[k, i]*B_transposed_tiled[j, k], axis=k),name='C_transposed_skipped')\n",
        "  \n",
        "  s = te.create_schedule(C_transposed_skipped.op)\n",
        "\n",
        "  print(C_transposed_skipped.op.axis)\n",
        "  # print(s[C_transposed_skipped].reduce_axis)\n",
        "\n",
        "get_tiled_matmul_kernel()"
      ],
      "metadata": {
        "id": "KiFCg_1wENnF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5546c353-b374-4b6b-f224-fd8ef2242c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[T.iter_var(j, T.Range(0, 32), \"DataPar\", \"\"), T.iter_var(i, T.Range(0, 1024), \"DataPar\", \"\")]\n"
          ]
        }
      ]
    }
  ]
}