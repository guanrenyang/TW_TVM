{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guanrenyang/TW_TVM/blob/main/tile_matmul.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: LD_LIBRARY_PATH=/usr/local/cuda/lib\n",
            "env: PATH=/usr/local/cuda/bin:/usr/bin\n"
          ]
        }
      ],
      "source": [
        "%env LD_LIBRARY_PATH=/usr/local/cuda/lib\n",
        "%env PATH=/usr/local/cuda/bin:/usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4pdug43brs7y"
      },
      "outputs": [],
      "source": [
        "import tvm\n",
        "import tvm.testing\n",
        "from tvm import te\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AnyVD9V2wXc7"
      },
      "outputs": [],
      "source": [
        "tgt_gpu = tvm.target.Target(target=\"cuda\", host=\"llvm\")\n",
        "gpu_0 = tvm.device(tgt_gpu.kind.name, 0)\n",
        "\n",
        "tgt_cpu = tvm.target.Target(target=\"llvm\", host=\"llvm\")\n",
        "cpu = tvm.device(tgt_cpu.kind.name, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EzSWohp5waHj"
      },
      "outputs": [],
      "source": [
        "M = 32\n",
        "N = 32\n",
        "K = 16\n",
        "tile_size = \n",
        "#int, N: int, K:int, K_pruned_max: int, N_pruned:int, tile_size:int,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AVLvig5wsqT",
        "outputId": "ea682443-bd6e-4b8d-8802-aff439322eb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 4, 6, 7, 8, 12, 13, 14]\n"
          ]
        }
      ],
      "source": [
        "def mask_gen(N: int, N_pruned: int, base = 0):\n",
        "    '''A tool to generate a mask (in the type of python list) given the number of \n",
        "    origial elements `N` and that of remaining elements `N_pruned`'''\n",
        "    mask_keep = list(range(N))\n",
        "    random.shuffle(mask_keep) # shuffle is an in-place operation\n",
        "    mask_keep = mask_keep[ : N_pruned]\n",
        "    mask_keep.sort()\n",
        "    for i, _ in enumerate(mask_keep):\n",
        "        mask_keep[i] = mask_keep[i]  + base\n",
        "    return mask_keep\n",
        "'''\n",
        "In the cuda version of TW, `mask[i]+i` is the real index relative to the beginning of its block of the corresponding element\n",
        "\n",
        "In this tvm version, we set `mask[i]` to be the real index instead of `mask[i]+i`.\n",
        "'''\n",
        "print(mask_gen(16, 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkPZNprpxXaS",
        "outputId": "6370cb13-171f-402b-daf4-ae28191d8de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "block_num_verify: 4\n",
            "N_original_perBlock_verify: 4\n",
            "mask_k_list_verify: [[1, 4, 6, 7], [0, 1, 2, 5], [2, 4, 5, 7], [2, 3, 5, 6]]\n",
            "mask_n_list_verify: [[1, 2], [0, 3], [1, 3], [0, 1]]\n",
            "\n",
            "Block of dense matrix:\n",
            " [[0.7338991  0.9039647  0.64008491 0.93956132]\n",
            " [0.30641148 0.19852933 0.52610446 0.96497124]\n",
            " [0.83806996 0.41329319 0.86986179 0.64806269]\n",
            " [0.01819067 0.86111985 0.45010897 0.7578844 ]\n",
            " [0.4083653  0.53162131 0.32541112 0.35413239]\n",
            " [0.54418759 0.51079587 0.32296146 0.19155029]\n",
            " [0.47227041 0.64321622 0.25844482 0.73020564]\n",
            " [0.82890208 0.53941138 0.32041523 0.42371766]]\n",
            "\n",
            "Block of sparse matrix\n",
            " [[0.19852933 0.52610446]\n",
            " [0.53162131 0.32541112]\n",
            " [0.64321622 0.25844482]\n",
            " [0.53941138 0.32041523]]\n"
          ]
        }
      ],
      "source": [
        "def get_B_Stream(B, mask_k_list, mask_n_list, block_num, N_pruned_perBlock, K_pruned, N_original):\n",
        "    ''' To generate a python list of B tiles with type of `list(tvm.nd.NDArray)`\n",
        "    Parameters:\n",
        "    * B(tvm.nd.NDArray): the undivided matrix B\n",
        "    * mask_k_list: mask of K dimension of each block with type of `list(list(int))`\n",
        "    * mask_n_list: mask of N dimension of each block with type of `list(list(int))`\n",
        "    * block_num: the number of blocks in B\n",
        "    * N_pruned_perBlock: the number of remaining elements in the N dimension of each block\n",
        "    * K_pruned: the number of remaining elements in the K dimension of each block\n",
        "    * N_original: the number of elements in the N dimension of the undivided matrix\n",
        "                    used to compute the offset of each block in the N dimension \n",
        "    '''\n",
        "    B_transposed_tiled_list = []\n",
        "    for bn in range(block_num):\n",
        "        mask_k = mask_k_list[bn]\n",
        "        mask_n = mask_n_list[bn]\n",
        "        \n",
        "        dst = np.zeros((N_pruned_perBlock, K_pruned))\n",
        "        for i in range(K_pruned):\n",
        "            for j in range(N_pruned_perBlock):\n",
        "                idx_col = mask_k[i] \n",
        "                idx_row = mask_n[j] + N_original * bn\n",
        "                dst[j, i] = B[idx_col, idx_row]\n",
        "        B_transposed_tiled_list.append(dst)\n",
        "    return B_transposed_tiled_list\n",
        "\n",
        "'''`_verify` means the variable is used for unit test'''\n",
        "B_verify = np.random.random((8, 16))\n",
        "K_verify = 8\n",
        "N_verify = 16\n",
        "\n",
        "K_pruned_verify = 4 # the numbers of remaining elements in the K dimension of each block are the same\n",
        "N_pruned_global_verify = 8 # but those in the N dimension are different\n",
        "\n",
        "tilesize_verify = 2\n",
        "\n",
        "block_num_verify = (N_pruned_global_verify+tilesize_verify-1)//tilesize_verify\n",
        "N_original_perBlock_verify = N_verify // block_num_verify\n",
        "print(\"block_num_verify:\",block_num_verify)\n",
        "print(\"N_original_perBlock_verify:\", N_original_perBlock_verify)\n",
        "\n",
        "mask_k_list_verify = [mask_gen(K_verify, K_pruned_verify) for _ in range(block_num_verify)]\n",
        "mask_n_list_verify = [mask_gen(N_original_perBlock_verify, tilesize_verify) for _ in range(block_num_verify)]\n",
        "print(\"mask_k_list_verify:\", mask_k_list_verify)\n",
        "print(\"mask_n_list_verify:\", mask_n_list_verify)\n",
        "\n",
        "B_transposed_tiled_list = get_B_Stream(B_verify, mask_k_list_verify,\\\n",
        "                                                mask_n_list_verify, block_num_verify, \\\n",
        "                                                tilesize_verify, K_pruned_verify, N_original_perBlock_verify)\n",
        "block_to_check = 0# number of block to check, from 0 to 3\n",
        "print(\"\\nBlock of dense matrix:\\n\", B_verify[0:8, 8*block_to_check:8*block_to_check+4])\n",
        "print(\"\\nBlock of sparse matrix\\n\", B_transposed_tiled_list[block_to_check].T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJpp0CGLBzdj",
        "outputId": "cdcb8c6d-2cae-4d0a-8e5c-394dbaf2ba5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Default schedule\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for j, i in T.grid(32, 1024):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j * 1024 + i] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_1: T.int32 = j * 1024 + i\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + i] * B_tiled_1[j * 128 + k]\n",
            "\n",
            "After split\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for i_outer, j_inner, i_inner in T.grid(32, 32, 32):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j_inner * 1024 + i_outer * 32 + i_inner] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_2: T.int32 = i_outer * 32\n",
            "                cse_var_1: T.int32 = j_inner * 1024 + cse_var_2 + i_inner\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + cse_var_2 + i_inner] * B_tiled_1[j_inner * 128 + k]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(schedule(0x2e68f70),\n",
              " [Tensor(shape=[32, 1024], op.name=C_transposed_skipped),\n",
              "  Tensor(shape=[2048, 1024], op.name=A_transposed),\n",
              "  Tensor(shape=[32, 128], op.name=B_tiled),\n",
              "  Tensor(shape=[128], op.name=mask_k)])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "M = 1024\n",
        "K = 1024\n",
        "N = 1024\n",
        "tile_size = 32\n",
        "K_pruned = 128\n",
        "N_pruned = 128\n",
        "\n",
        "\n",
        "block_num = (N_pruned + tile_size - 1) // tile_size\n",
        "N_ori = N//block_num\n",
        "def get_tiled_matmul_kernel(cuda=False):\n",
        "    '''TW Tiled-Gemm kernel\n",
        "    Input of the kernel: \n",
        "    * A_transposed\n",
        "    * B_transposed_tiled\n",
        "    * mask_k\n",
        "    Output of the kernel:\n",
        "    * '''\n",
        "    A_transposed = te.placeholder((K, M), name='A_transposed')\n",
        "    mask_k = te.placeholder((K_pruned,), name='mask_k', dtype='int')\n",
        "    B_transposed_tiled = te.placeholder((tile_size, K_pruned), name='B_tiled')\n",
        "    # mask_k = [0 for i in range(K_pruned)]\n",
        "    # mask_n = te.placeholder((N_dim,), name=\"mask_n\")\n",
        "    \n",
        "    A_transposed_skipped = te.compute((K_pruned, M), lambda i,j: A_transposed[mask_k[i], j], name='A_skipped')\n",
        "\n",
        "    k = te.reduce_axis((0, K_pruned), name='k')\n",
        "    C_transposed_skipped = te.compute((tile_size, M),lambda j,i: te.sum(A_transposed_skipped[k, i]*B_transposed_tiled[j, k], axis=k),name='C_transposed_skipped')\n",
        "    \n",
        "    s = te.create_schedule(C_transposed_skipped.op)\n",
        "\n",
        "    '''schedule'''\n",
        "    print('\\nDefault schedule')\n",
        "    print(tvm.lower(s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k], simple_mode=True))\n",
        "\n",
        "    yo, xo, yi, xi = s[C_transposed_skipped].tile(C_transposed_skipped.op.axis[0], C_transposed_skipped.op.axis[1], x_factor = 32, y_factor=32)\n",
        "    print('\\nAfter split')\n",
        "    print(tvm.lower(s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k], simple_mode=True))\n",
        "\n",
        "    if cuda:\n",
        "        s[C_transposed_skipped].bind(yo, te.thread_axis(\"blockIdx.y\"))\n",
        "        s[C_transposed_skipped].bind(xo, te.thread_axis(\"blockIdx.x\"))\n",
        "        s[C_transposed_skipped].bind(yi, te.thread_axis(\"threadIdx.y\"))\n",
        "        s[C_transposed_skipped].bind(xi, te.thread_axis(\"threadIdx.x\"))\n",
        "        print('\\nLaunch threads')\n",
        "        print(tvm.lower(s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k], simple_mode=True))\n",
        "    \n",
        "    return s, [C_transposed_skipped, A_transposed, B_transposed_tiled, mask_k]\n",
        "get_tiled_matmul_kernel()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Default schedule\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for j, i in T.grid(32, 1024):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j * 1024 + i] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_1: T.int32 = j * 1024 + i\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + i] * B_tiled_1[j * 128 + k]\n",
            "\n",
            "After split\n",
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed_skipped: T.Buffer((32, 1024), \"float32\"), A_transposed: T.Buffer((2048, 1024), \"float32\"), B_tiled: T.Buffer((32, 128), \"float32\"), mask_k: T.Buffer((128,), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_skipped = T.allocate([131072], \"float32\", \"global\")\n",
            "        A_skipped_1 = T.Buffer((131072,), data=A_skipped)\n",
            "        for i, j in T.grid(128, 1024):\n",
            "            A_transposed_1 = T.Buffer((2097152,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((128,), \"int32\", data=mask_k.data)\n",
            "            A_skipped_1[i * 1024 + j] = A_transposed_1[mask_k_1[i] * 1024 + j]\n",
            "        for i_outer, j_inner, i_inner in T.grid(32, 32, 32):\n",
            "            C_transposed_skipped_1 = T.Buffer((32768,), data=C_transposed_skipped.data)\n",
            "            C_transposed_skipped_1[j_inner * 1024 + i_outer * 32 + i_inner] = T.float32(0)\n",
            "            for k in range(128):\n",
            "                cse_var_2: T.int32 = i_outer * 32\n",
            "                cse_var_1: T.int32 = j_inner * 1024 + cse_var_2 + i_inner\n",
            "                B_tiled_1 = T.Buffer((4096,), data=B_tiled.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_skipped_1[k * 1024 + cse_var_2 + i_inner] * B_tiled_1[j_inner * 128 + k]\n",
            "C_transposed_skipped [32, 1024]\n",
            "A_transposed [2048, 1024]\n",
            "B_tiled [32, 128]\n",
            "mask_k [128]\n"
          ]
        }
      ],
      "source": [
        "'''Testing cpu'''\n",
        "schedule, placeholders = get_tiled_matmul_kernel(cuda=False)\n",
        "for ph in placeholders:\n",
        "  print(ph.op.name, ph.shape)\n",
        "\n",
        "tiled_matmul_kernel = tvm.build(schedule, placeholders, target=tgt_cpu, name=\"tiled_matmul\")  \n",
        "A_transposed_data = tvm.nd.array(np.random.uniform(size=(K, M)).astype(placeholders[1].dtype), cpu)\n",
        "B_transposed_tiled_data = tvm.nd.array(np.random.uniform(size=(tile_size, K_pruned)).astype(placeholders[2].dtype), cpu)\n",
        "mask_k_data = tvm.nd.array(np.array(mask_gen(K, K_pruned)).astype(placeholders[3].dtype), cpu)\n",
        "\n",
        "C_transposed_skipped_data = tvm.nd.array(np.random.uniform(size=(tile_size, M)).astype(placeholders[0].dtype), cpu)\n",
        "\n",
        "tiled_matmul_kernel(C_transposed_skipped_data, A_transposed_data, B_transposed_tiled_data, mask_k_data)\n",
        "\n",
        "def tiled_matmul_test(A_transposed, B_transposed_tiled, mask_k):\n",
        "    A_transposed_skipped = np.zeros((K_pruned, M))\n",
        "    for i in range(K_pruned):\n",
        "        for j in range(M):\n",
        "            A_transposed_skipped[i, j] = A_transposed[mask_k[i], j]\n",
        "    C_transposed_skipped = (A_transposed_skipped.T @ B_transposed_tiled.T).T\n",
        "    return C_transposed_skipped\n",
        "\n",
        "tvm.testing.assert_allclose(C_transposed_skipped_data.numpy(), tiled_matmul_test(A_transposed_data.numpy(), B_transposed_tiled_data.numpy(), mask_k_data.numpy()), 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(C_transposed: T.Buffer((1024, 1024), \"float16\"), A_transposed: T.Buffer((1024, 1024), \"float32\"), B_transposed_packed: T.Buffer((16, 32, 128), \"float32\"), mask_k: T.Buffer((16, 128), \"int32\"), mask_n: T.Buffer((16, 32), \"int32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True})\n",
            "        A_transposed_skipped = T.allocate([2097152], \"float16\", \"global\")\n",
            "        C_transposed_skipped = T.allocate([524288], \"float16\", \"global\")\n",
            "        A_transposed_skipped_1 = T.Buffer((2097152,), \"float16\", data=A_transposed_skipped)\n",
            "        for bn, i, j in T.grid(16, 128, 1024):\n",
            "            A_transposed_1 = T.Buffer((1048576,), data=A_transposed.data)\n",
            "            mask_k_1 = T.Buffer((2048,), \"int32\", data=mask_k.data)\n",
            "            A_transposed_skipped_1[bn * 131072 + i * 1024 + j] = T.Cast(\"float16\", A_transposed_1[mask_k_1[bn * 128 + i] * 1024 + j])\n",
            "        C_transposed_skipped_1 = T.Buffer((524288,), \"float16\", data=C_transposed_skipped)\n",
            "        for bn, j, i in T.grid(16, 32, 1024):\n",
            "            C_transposed_skipped_1[bn * 32768 + j * 1024 + i] = T.float16(0)\n",
            "            for k in range(128):\n",
            "                cse_var_1: T.int32 = bn * 32768 + j * 1024 + i\n",
            "                B_transposed_packed_1 = T.Buffer((65536,), data=B_transposed_packed.data)\n",
            "                C_transposed_skipped_1[cse_var_1] = C_transposed_skipped_1[cse_var_1] + A_transposed_skipped_1[bn * 131072 + k * 1024 + i] * T.Cast(\"float16\", B_transposed_packed_1[bn * 4096 + j * 128 + k])\n",
            "        T.attr(0, \"extern_scope\", 0)\n",
            "        C_transposed_1 = T.Buffer((1048576,), \"float16\", data=C_transposed.data)\n",
            "        for n, m in T.grid(1024, 1024):\n",
            "            C_transposed_1[n * 1024 + m] = T.float16(0)\n",
            "        for bn, ts, col in T.grid(16, 32, 1024):\n",
            "            cse_var_2: T.int32 = bn * 32768\n",
            "            mask_n_1 = T.Buffer((512,), \"int32\", data=mask_n.data)\n",
            "            C_transposed_1[cse_var_2 + mask_n_1[ts] * 1024 + col] = C_transposed_1[cse_var_2 + mask_n_1[ts] * 1024 + col] + C_transposed_skipped_1[cse_var_2 + ts * 1024 + col]\n"
          ]
        }
      ],
      "source": [
        "# N_pruned is the number of remaining entries in N dimension\n",
        "# TODO: change K_pruned to a layer-wise configuration\n",
        "def get_tw_kernel( M: int, N: int, K:int, K_pruned_max: int, N_pruned_global:int, tile_size:int,cuda:bool=False):\n",
        "    '''TW Tiled-Gemm kernel\n",
        "    Input of the kernel: \n",
        "    * A: K*M\n",
        "    * B: (block_num, tile_size, K_pruned_max)\n",
        "    * C: N*M\n",
        "    * mask_k: (block_num, K_pruned_max)\n",
        "    * mask_n: (block_num, tile_size)\n",
        "    * block_num\n",
        "    Output of the kernel:\n",
        "    * '''\n",
        "    dtype = 'float16'\n",
        "    block_num = (N_pruned_global + tile_size - 1)//tile_size\n",
        "    N_ori_per_block = N // block_num\n",
        "\n",
        "    A_transposed = te.placeholder((K, M), name='A_transposed')\n",
        "    B_transposed_packed = te.placeholder((block_num, tile_size, K_pruned_max), name='B_transposed_packed')\n",
        "    \n",
        "\n",
        "    mask_k = te.placeholder((block_num, K_pruned_max), name='mask_k', dtype='int')\n",
        "    mask_n = te.placeholder((block_num, tile_size), name='mask_n', dtype='int') # \n",
        "\n",
        "    A_transposed_skipped = te.compute((block_num, K_pruned_max, M), lambda bn, i, j: A_transposed[mask_k[bn, i], j].astype(dtype), name='A_transposed_skipped')\n",
        "    \n",
        "    k = te.reduce_axis((0, K_pruned_max), name='k')\n",
        "    C_transposed_skipped = te.compute((block_num, tile_size, M), lambda bn, j, i: te.sum(A_transposed_skipped[bn, k, i] * B_transposed_packed[bn, j, k].astype(dtype), axis=k) , name='C_transposed_skipped')\n",
        "\n",
        "    def write_C_to_sparse(data, mask_n, out):\n",
        "        '''\n",
        "        data: shape of (block_num, tile_size, M)\n",
        "        mask_n: shape of (block_num, tile_size)\n",
        "        '''\n",
        "        irb = tvm.tir.ir_builder.create()\n",
        "        data_ptr = irb.buffer_ptr(data)\n",
        "        mask_n_ptr = irb.buffer_ptr(mask_n)\n",
        "        out_ptr = irb.buffer_ptr(out)\n",
        "\n",
        "        assert data.shape[0]==mask_n.shape[0], 'block_num mismatches'\n",
        "        block_num = data.shape[0]\n",
        "        assert data.shape[1]==mask_n.shape[1], 'tile_size mismatches'\n",
        "        tile_size = data.shape[1]\n",
        "        \n",
        "        N = out.shape[0]\n",
        "        M = out.shape[1]\n",
        "\n",
        "        with irb.for_range(0, N, kind='serial', name='n') as n:\n",
        "            with irb.for_range(0, M, kind='serial', name='m') as m:\n",
        "                out_ptr[n * M + m] = tvm.tir.generic.cast(0, data.dtype)\n",
        "\n",
        "        with irb.for_range(0, block_num, kind='serial', name='bn') as bn:\n",
        "            with irb.for_range(0, tile_size, kind='serial', name='ts') as ts:\n",
        "                with irb.for_range(0, M, kind='serial', name='col') as col:\n",
        "                    out_ptr[(tile_size * bn + mask_n_ptr[ts]) * M + col] += data_ptr[bn * tile_size * M + ts * M + col]\n",
        "        return irb.get()\n",
        "        \n",
        "    C_transposed = te.extern((N, M),\n",
        "                             [C_transposed_skipped, mask_n],\n",
        "                             lambda ins, outs: write_C_to_sparse(ins[0], ins[1], outs[0]),\n",
        "                             tag='write_C_to_sparse',\n",
        "                             dtype=C_transposed_skipped.dtype,\n",
        "                             name='C_transposed',\n",
        "                             )\n",
        "    \n",
        "    s = te.create_schedule(C_transposed.op)\n",
        "\n",
        "    '''testing cpu'''\n",
        "    func = tvm.build(s, [C_transposed, A_transposed, B_transposed_packed, mask_k, mask_n], tgt_cpu, name='tiled_matmul')\n",
        "    A_transposed_test = tvm.nd.array(np.random.random((K, M)).astype(A_transposed.dtype), cpu)\n",
        "    B_transposed_packed_test = tvm.nd.array(np.random.random((block_num, K_pruned_max, M)).astype(B_transposed_packed.dtype), cpu)\n",
        "    C_transposed_test = tvm.nd.array(np.random.random((N, M)).astype(C_transposed.dtype), cpu)\n",
        "\n",
        "    \n",
        "    print(tvm.lower(s, [C_transposed, A_transposed, B_transposed_packed, mask_k, mask_n], simple_mode=True))\n",
        "    \n",
        "    \n",
        "get_tw_kernel(1024, 1024, 1024, 128, 512, 32)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOsXw8Qb1X5fa8km6wQCAiz",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "tvm-build-binding",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "400f399043f899df7d546bcb6063b1538c013427ed509bbd25e6c99d0cce4f96"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
